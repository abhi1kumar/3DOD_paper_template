\documentclass[12pt]{article}
%\usepackage[utf8]{inputenc}

\usepackage[margin=0.97in]{geometry}
\usepackage{graphicx}
% DO NOT USE \usepackage{times}, it will be removed by typesetters
%\usepackage{times}

\usepackage{comment}
\usepackage{amsmath,amssymb} % define this before the line numbering.
\usepackage{color}

% The "axessiblity" package can be found at: https://ctan.org/pkg/axessibility?lang=en
\usepackage[accsupp]{axessibility}  % Improves PDF readability for those with disabilities.


%============================================================================
% Include other packages here
%============================================================================
\usepackage{graphicx}
%\usepackage{amsthm}           % for proof
\usepackage{booktabs}

\usepackage{epsfig}
\usepackage{tabularx}
\usepackage[table]{xcolor}       % for colored stuff
% \usepackage{verbatim}            % For multiline comments
\usepackage{booktabs}
\usepackage{relsize}
\usepackage{multirow}
\usepackage{scrextend}           % Put footmark in the bottom of the page
\usepackage{array}               % Thicker vertical rules in tables, lines in only certain columns
\usepackage{makecell}            % For thicker horizontal rules in tables
\usepackage{subcaption}          % For subfigures
\usepackage{wasysym}             % For right filled arrow
\usepackage{lipsum}              % For random filler text
\usepackage{esvect}              % For vv command arrow above symbols
\usepackage{soul}                % For striked text command \st and hightlight
\usepackage{float}
\usepackage{caption}
\usepackage{bm}
\usepackage{cite}                % orders the citations in increasing order
\usepackage{paralist}            % for compactitem
\usepackage{enumitem}            % for noindent itemize
\usepackage{empheq}
\usepackage{gensymb}             % for degree symbol
\usepackage{wrapfig}             % for wraptable
\usepackage{xspace}              % for dynamic controlled space at end of abbreviation

\definecolor{lightgreen}{HTML}{90EE90}
\newcommand{\coloredeq}[2]{\begin{empheq}[box=\colorbox{lightgreen}]{align}\label{#1}#2\end{empheq}}
\newcommand{\NParagraph}[1]{\vspace{0.6mm} \noindent \textbf{#1}
\hspace{0mm}}

\captionsetup{font=small}        % Figures and Tables would have small captions 
\allowdisplaybreaks              % Break aligned equations

\graphicspath{{images/}}

%===============================================================================
% For diagrams in Latex
%===============================================================================
% https://tex.stackexchange.com/a/398269
\makeatletter
\@namedef{ver@everyshi.sty}{}
\makeatother
\usepackage{tikz} % for some figures
\usetikzlibrary{calc}  % for centerarc
\usetikzlibrary{arrows}
\usetikzlibrary{arrows.meta}
\usetikzlibrary{shapes.arrows}
\usetikzlibrary{fadings, shadows}
\usetikzlibrary{decorations.text}
\def\centerarc[#1](#2)(#3:#4:#5)% Syntax: [draw options] (center) (initial angle:final angle:radius)
    { \draw[#1] ($(#2)+({#5*cos(#3)},{#5*sin(#3)})$) arc (#3:#4:#5); }


\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}

\input{notations}

\newcommand{\revOne}{{\color{red}{R1}}}
\newcommand{\revTwo}{{\color{orange}{R2}}}
\newcommand{\revThree}{{\color{my_magenta}{R3}}}
\newcommand{\revFour}{{\color{my_blue}{R4}}}



\usepackage[pagebackref,breaklinks,colorlinks,citecolor={my_blue},urlcolor=black]{hyperref}
% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%\title{ Simultaneous Localization and Mapping for Dynamic Scenes}
%\author{Xiaoming Liu}
\date{} % clear date

%============================================================================
%============================================================================
%============================================================================
\begin{document}



\pagenumbering{arabic}% resets `page` counter to 1
\renewcommand*{\thepage}{Page \arabic{page}}
%============================================================================
%============================================================================
%============================================================================
% \paragraph{Reviewer 1:}
% Hi Abhinav, the response is very coarse, just for an outline
% \abhinav{OK Shengjie}
% \begin{itemize}
%     \item 
%     \item Thanks for expressing interest in the multi-frame 2D bundle adjustment! 
%     Our major assumption is 2D based bundle adjustment can be more accurate before involving noisy 3D estimation.
%     \item The pixel-wise motion estimation is mostly enforeced during training. There exists a pipeline, feature map to pooling to single pixel to estimaiton.
%     \item We can include additional datasets. 
% \end{itemize}


We thank the reviewers for three strong accepts and their helpful feedback, which we will incorporate into the project plan, if this project is funded. We are happy that reviewers consider this as a well-justified (\revTwo), meritorious (\revFour), feasible (\revThree, \revFour) and technically detailed (\revFour) proposal with innovative concepts (\revFour) and incredibly interesting multi-frame \twoD  bundle adjustment (\revOne). 
We now address the individual concerns from all reviewers, as follows.
% that tackles an important topic in \threeD  monocular vision (\reviewerFour), with lot of experiments (\revOne) and very good/competitive results (\reviewerTwo/\reviewerFour).

%===============================================================================
% Reviewer 1
%===============================================================================

    %===============================================================================
    \textbf{\revOne, Q1: Explicitly encoding scale in a network will mean that individual layers will most likely need many more parameters than their conventional counterparts. However, it is also possible that scale equivariant networks would require fewer overall layers and may in part (or completely?) negate the
layer wise parameter increases.}

    \ScaleEquivariant networks contain same trainable parameters \cite{sosnovik2020sesn} by design.
    Such networks contain non-learnable kernels at multiple scales, which they linearly combine with learnable weights to mimic the vanilla convolution operation. 
    However, because of the linear combination, the filters at different scales are constrained making these networks \scaleEquivariant. 
    \scaleEquivariant networks keep the number of learnable weights exactly same as the number of trainable parameters in vanilla networks and hence, both have same number of trainable parameters.
    
    We also expect \scaleEquivariant networks to require fewer overall layers since they have the right inductive bias.

    %===============================================================================
    \textbf{\revOne, Q2: The proposersâ€™ formulation of multi frame \twoD bundle adjustment (BA) is incredibly
interesting.}

    Thanks for the encouraging comment on multi-frame \twoD bundle adjustment! 
    Our major assumption is \twoD based bundle adjustment can be more accurate before involving noisy \threeD estimation.

    %===============================================================================
    \textbf{\revOne, Q3: Instance wise and pixel-wise object motion estimation is a potentially useful approach in terms of accuracy but could be very slow}.

    We want to point out that for both two-stage and one-stage object detectors, the algorithms typically execute a dense initial prediction, regressing initial pixel-wise objects' scores and object 3D poses. 
    Thus, regressing pixel-wise poses does not impact inference speed heavily. 
    We do not agree inference speed will be impacted due to pixel-wise object pose estimation. 
    In object detectors, there exists a pixel-wise dense pose / objectness score initial estimation, conducted at an $8\times$ downsampled image resolution. 
    The pixel-wise object pose is popularly adopted in recent object detectors.
    For pixel-wise object motion, as in main paper Eqn.~$6$, it is computed using pixel-wise object pose at frame $\mathbf{I}_t$ and frame $\mathbf{I}_{t+1}$.
    By interpolating pixel-wise object pose at frame $\mathbf{I}_{t+1}$ using optical flow, we can acquire $\mathbf{P}_{t+1}^O$.
    Then, the pixel-wise object motion is computed as: $\mathbf{P}_{t, t+1}^O = (\mathbf{P}_{t}^O)^{-1}\mathbf{P}_{t+1}^O$
    
    
    %===============================================================================
    \textbf{\revOne, Q4: For evaluating the entire pipeline in Task 4, I am unsure if KITTI has enough dynamic motion to warrant such an expensive object motion approach. I would advise to look for a dataset with more dynamic motion. I would also advise an ablation study to evaluate the effect of the object motion module.}

    For outdoor scenes, we will include \argoverseTwo \cite{wilson2023argoverse} dataset in addition to \kitti for more complicated object movements.
    For indoor scenes, we can generalize our detector to detect furniture, \eg, chairs, computers, desks, and \textit{etc.}.
    This enables us to evaluate our method on a dataset including complicated self-motion, \eg, \scannet~\cite{dai2017scannet} dataset.
    Also see \revThree, Q1.

\clearpage
%===============================================================================
% Reviewer 2
%===============================================================================

    %===============================================================================
    \textbf{\revTwo, Q1: The approach described in the proposal of replacing the vanilla convolutions by their scale equivariant counterparts, implementing a two-stage camera pose estimation algorithm that includes camera pose optimization and camera scale estimation stages, and an end to end trainable system using monocular object detection and bundle adjustment is unique and appears to be well justified. I am interested in seeing the work completed and compared to the current state of the art.}

    Thanks! We are equally excited to work on this project as well.

\clearpage
%===============================================================================
% Reviewer 3
%===============================================================================

    %===============================================================================
    \textbf{\revThree, Q1: I am a little unsure of what rating to give since I donâ€™t know exactly what TRL the end goal of the program that supports the work is. The proposal proposes pure computer vision algorithm research on a set of on road dataset proposal and will propose good contributions in that domain but not contribute a lot to several of the Armyâ€™s problems of operating without GPS in dynamic and challenging situations without significantly more work to adapt and evaluate the methods to relevant scenarios. \\
    If basic research for computer vision algorithms contributions evaluated on on-road datasets is acceptable I would say accept. Regardless it might be good to communicate some of the comments especially regarding train and test on \kitti to the PI.
    If at the end a functional robust algorithm evaluated for Army relevant situations is required, I would say decline.
}

        \begin{table}[!tb]
            \caption{\textbf{Datasets} and their usage. [Key: Det.= Detection, Illu.= Illumination, Comp. Ego Movement= Ego Movement, *= Do not exist]}
            \label{tab:datasets}
            \centering
            \scalebox{\scaleFraction}
            {
            % \footnotesize
            % \setlength\tabcolsep{0.15cm}
            \begin{tabular}{m l t cc t cc t cccc t c c|cm}
                \myTopRule
                \multirow{2}{*}{\textbf{Dataset} {\raisebox{0.1\normalbaselineskip}{\scalebox{0.85}{\downarrowRHD}}} } & \multirow{2}{*}{Train} & \multirow{2}{*}{Test} & \multicolumn{2}{ct}{Task} & \multicolumn{4}{ct}{Weather} & 
                \multirow{2}{*}{Night} & {Comp. Ego}\\
                \cline{4-9}
                &&& Det. & SLAM & Rain & Dust & Fog & Snow && Movement\\
                \myTopRule
                \kitti \cite{geiger2012we}                & \cmark    & \cmark & \cmark    & \cmark    & \mathDash & \mathDash & \mathDash & \mathDash & \mathDash & \mathDash\\
                \waymo \cite{sun2020scalability}          & \cmark    & \cmark & \cmark    & \cmark    & \cmark    & \cmark    & \cmark    & \mathDash & \cmark    & \mathDash  \\
                \nuscenes \cite{caesar2020nuscenes}       & \mathDash & \cmark & \cmark    & \cmark    & \cmark    & \mathDash & \mathDash & \mathDash & \cmark    & \mathDash \\
                Adv Weather\cite{bijelic2020seeing}       & \mathDash & \cmark & \cmark    & \mathDash & \cmark    & \mathDash & \cmark    & \cmark    & \cmark    & \mathDash \\
                \ithaca\cite{diaz2022ithaca365}           & \mathDash & \cmark & \cmark    & \cmark    & \cmark    & \mathDash & \cmark    & \cmark    & \cmark    & \mathDash\\
                \argoverseTwo  \cite{wilson2023argoverse} & \cmark    & \cmark & \mathDash & \cmark    & \mathDash & \mathDash & \mathDash & \mathDash & \cmark & \mathDash \\
                \rugd \cite{weigness2019rugd}             & \mathDash & \cmark & \mathDash & \cmark    & \multicolumn{4}{c|}{\text{Unconstrained Off-Road}} & \cmark    & \cmark\\
                MSU Canvas*                               & \mathDash & \cmark & \mathDash & \cmark    & \cmark    & \cmark    & \cmark    & \cmark    & \cmark    & \cmark\\
                Shaky \kitti *                            & \mathDash & \cmark & \mathDash & \cmark    & \cmark    & \cmark    & \cmark    & \cmark    & \cmark    & \cmark\\
                \myTopRule
            \end{tabular}
            }
        \end{table}

    Great point! We plan to evaluate our system from the following perspectives:
    \begin{compactitem}
    \item \textbf{Cross-Dataset Generalization.} Cross-dataset generalization forces our methods to \emph{think} beyond memorization on a single dataset and, therefore, better cross-dataset models are in general more generalizable models. Hence, we aim for \nuscenes \cite{caesar2020nuscenes}, \advWeather \cite{bijelic2020seeing} and \ithaca \cite{diaz2022ithaca365} datasets in cross-dataset evaluation for object detection, and \rugd \cite{weigness2019rugd} for cross-dataset evaluation of SLAM
    
    \item \textbf{Weather Robustness.} The image-based methods are sensitive to weather conditions \cite{sun2020scalability} and therefore, we plan to benchmark under challenging conditions of raind, dust, snow and fog. Since none of the publicly available datasets contain all these weather conditions, we plan to benchmark on \waymo \cite{sun2020scalability}, \nuscenes \cite{caesar2020nuscenes}, \advWeather \cite{bijelic2020seeing} and \ithaca \cite{diaz2022ithaca365} datasets.
    
    \item \textbf{Illumination Robustness.} The image-based methods are also sensitive to illumination conditions \cite{kumar2019vpds}, and therefore, we will also benchmark on sub-splits of \waymo \cite{sun2020scalability}, \nuscenes \cite{caesar2020nuscenes}, \advWeather \cite{bijelic2020seeing} and \ithaca \cite{diaz2022ithaca365} for object detection, and \rugd for SLAM.
    
    \item \textbf{Ego motion Robustness.} We will evaluate our algorithm without the dynamic object detection module on unconstrained outdoor environment \rugd \cite{weigness2019rugd} dataset.
    Finally, we have access to MSU CANVAS cars\footnote{\url{https://canvas.msu.edu/mobility-studio}}, which can be used for our own data collection that is more closer to the Army relevant scenario. 
    For example, at MSU campus, we have considerate amount of unpaved road around experimental farm fields. 
    In the winter months, driving on uneven road surfaces and heavy snowy is another scenario. 
    Although this dataset is not yet curated, we can consider making a dataset for the same
    Another alternative is to re-construct the static \threeD scene for \kitti and then simulate the random ego shakes and generate the corresponding ground truth. We call this dataset Shaky \kitti. 
    \end{compactitem}
    
    We list out these datasets and their usage in \cref{tab:datasets}. 
    An important thing to note is any method does not become deployable from day one. One has to start with a method that works in constrained environments and then slowly remove the constraints after solid benchmarking.

    %===============================================================================
    \textbf{\revThree, Q2: On the high level the proposed methods are reasonable and the proposed methods haves a high chance of being successfully executed.
    }

    Thanks for your encouraging words!

    %===============================================================================
    \textbf{\revThree, Q3: It is proposed to train and evaluate on \kitti and \waymo datasets. This has been known to be insufficient to test learning based methods. The motion patterns in these datasets are extremely biased (Only forward car like motion from a fixed perspective) which the learning algorithms pick up and therefore learning based methods cannot be trained on these datasets unless the only objective is to overfit to that dataset. There are now several larger and more generalizable datasets especially focused on providing a large scale for learning based as well as dynamic object algorithms. Another example is that optical flow extraction as well as mapping to rotation/translation will overfit to car like motions in \kitti only datasets.}

    Great point! 
    % We do plan to evaluate in the cross-dataset setting with such datasets. 
    We want to comprehensively benchmark the algorithms from multiple perspectives on challenging benchmarks such as \nuscenes and \argoverseTwo. 
    We list all these datasets in \cref{tab:datasets} and detail our response in \revThree, Q1.
    % Cross-dataset generalization forces our methods to \emph{think} beyond memorization on a single dataset and, therefore, better cross-dataset models are in general more generalizable models. 
    % In addition, we also plan to evaluate larger challenging datasets as well.
    
    %We consider there lies an underlying confusion in our algorithm. 
    Dynamic object SLAM is dynamic in two-folds. 
    First, we investigate if the estimation of dynamic object improves the SLAM performance.
    Second, we desire a robust SLAM algorithm for dynamic scenes.
    The driving datasets are suitable for the first point since we require accurate detection of moving objects.
    But, we test the algorithm itself over the latter application case where we can include mentioned datasets like \rugd \cite{weigness2019rugd} and Bonn RGBD Dynamic \cite{tanke2019bonn} datasets.

    %===============================================================================
    \textbf{\revThree, Q4: Would ARL released datasets like \rugd dataset serve to contrast on-road, or insufficiently dynamic?)}
    
    % We can not use the \rugd dataset \cite{weigness2019rugd}. This dataset contains the semantic segmenetation labellings in cluttered environments but do not contain any bounding box annotations.
    We can use the \rugd dataset \cite{weigness2019rugd}. 
    However, the \rugd dataset contains the semantic segmentation labelings in cluttered environments but does not contain any bounding box annotations.
    Hence, we can test our algorithm without fusing dynamic object detection.

    %===============================================================================
    \textbf{\revThree, Q5: In general but especially for Army/DoD applications robustness is very important. The evaluation only considers performance metrics.}
    
    Thanks for bringing this up. 
    In addition to the standard metrics, we plan to benchmark on the following metrics.
    \begin{compactitem}
    \item \textbf{Object Detection.} We will use the weighted \ap metric to benchmark our algorithms. 
    The weighted \ap is given by
    \begin{align}
        \text{Weighted \ap} &= \frac{\sum\limits_{i} w_i \ap{}_i}{\sum{w_i}}
    \end{align},
    with the weight $w_i$ is $1$ if in a bright image, while it is $5$ for an image taken in rain, snow, dust or fog. 
    The Weighted \ap metric gives more importance to challenging images in the dataset and is inspired by the \nuscenes NDS metric \cite{caesar2020nuscenes} which is the weighted sum of a bunch of metrics including \ap and keeps the weight of \ap as $5$.
    
    \item \textbf{\slam .} We will also report RMSE based error-sensitive metric in addition to pose AUC curve based on degree error . 
    \end{compactitem}

    We also benchmark on a number of challenging datasets with diverse environments. 
    We list out these datasets in \cref{tab:datasets} and detail our response in \revThree, Q1.
    
    % We also report results on challenging subsets of Waymo (such as rain or dust) and evaluate in the cross-dataset setting. 
    % To additionally test robustness, we also plan to test monocular \threeD detectors trained on standard datasets on challenging datasets such as \advWeather~\cite{bijelic2020seeing} and \ithaca \cite{diaz2022ithaca365}. Also see \revThree, Q1.
    % In addition to reporting robust odometry estimation metric, \eg, pose AUC curve based on degree error, we will also report RMSE based error-sensitive metric.
    % Besides, we can test our algorithm on different datasets with diverse environments.
    

    %===============================================================================
    \textbf{\revThree, Q6: The proposal is missing some key related work on joint dynamic learning based SLAM (both learning and non learning) and some of the ideas proposed have been explored in these papers. For example several methods using cuboids, keypoints, etc. in the joint estimation and so on.
    }

    % How many do we need?
    Here is a list of important SLAM prior works:
    \begin{compactitem}
    \item EM-Fusion: Dynamic Object-Level SLAM With Probabilistic Data Association~\cite{strecke2019fusion}.
    \item RGBD SLAM in dynamic environments using point correlations~\cite{dai2020rgb}.
    \item DGS-SLAM: A fast and robust RGBD SLAM in dynamic environments combined by geometric and semantic information \cite{yan2022dgs}.
    \item RGBD SLAM in dynamic environments using point correlations \cite{dai2020rgb}.
    \end{compactitem}
    Again, the additional listed prior works focus on designing robust SLAM algorithms for dynamic objects, while we also desire improved \threeD object detection of the moving objects.
    On this point, the additionally listed papers are less relevant to our work.

    We will ablate the usage of cuboids and keypoints into the joint estimation part and see if they improve the overall accuracy of the pipeline.

    %===============================================================================
    \textbf{\revThree, Q7: The pipeline is separated into the various four tasks. Each of these tasks seems feasible. It would have been nice to see a formulation that combines everything in a joint framework that can at the end be jointly optimized.}

    % Thanks for the great suggestion! This is an exciting avenue for future work.
    We will!
    As stated in the proposal, after initialization of each module, we impose a learning-to-optimize scheme between object detection, self-pose estimation and optical flow estimation. 
    We emphasize that, training the task separately to get the initialization is important.
    %having each module initialized properly is important.
    since it is challenging to learn subtasks like optical flow, and self-pose estimation entirely on a single dataset like \kitti.
    Nevertheless, the joint optimization boosts consistency among the modules and improves test time accuracy. 
    Its supervision signal is still far behind the groundtruth label provided in each modality.

    %===============================================================================
    \textbf{\revThree, Q8: The amount of people and time period for the proposed task seems like a lot. It seems that it should be possible to execute roughly with 2 people in 1.5 years or another way of saying it would be to say that the scope seems a bit small.}

    % \abhinav{: I do not know how to answer this.}
    We respectfully disagree. Both detector and SLAM are well-studied and involved techniques. 
    However, merging and combining the codebases and thorough and robust evaluation requires time.

    %===============================================================================
    \textbf{\revThree, Q9: Overall the proposal is sound and feasible. I am not sure what the expect readiness level or level of basic research is expected for this call. If this is expected to be a basic research proposal with a focus on testing and developing some methods this does meet the criteria.
    }

    Thanks for echoing our approach. 

\clearpage
%===============================================================================
% Reviewer 4
%===============================================================================

    %===============================================================================
    \textbf{\revFour, Q1: Overall this is a meritorious investigation of visual ego and ado agent motion estimation for vehicles traveling in highly dynamic environments.}

    Thanks for your encouraging words!

    %===============================================================================
    \textbf{\revFour, Q2: However the field is busy and there are many other efforts, so it is important to leverage preexisting capabilities of others from industry and academia, to focus on the truly novel aspects of separating out the dynamic objects, estimating self motion, and then estimating the other objects (sometimes called ado agents).
    }

    We plan to carefully showcase our contributions and compare them with existing methods on benchmarks. 

    %===============================================================================
    \textbf{\revFour, Q3: Effective evaluation techniques will be critical, and the PIs should consider trying to obtain new, more challenging data sets beyond those used extensively already (such as \kitti).}

    Thanks for bringing this up. 
    We also benchmark on a number of challenging datasets with diverse environments. 
    We list out these datasets in \cref{tab:datasets} and detail our response in \revThree, Q1.
    % We also report results on challenging subsets of Waymo (such as rain or dust) and evaluate in the cross-dataset setting. 
    % To additionally test robustness, we also plan to test monocular \threeD detectors trained on standard datasets on challenging datasets such as \advWeather \cite{bijelic2020seeing} and \ithaca \cite{diaz2022ithaca365}. 
    % Also see \revThree, Q1.

    %===============================================================================
    \textbf{\revFour, Q4: The proposal has a number of innovative concepts that can advance SLAM capabilities.}

    Thanks for your encouraging words!

    %===============================================================================
    \textbf{\revFour, Q5: Historically there has been a lot of work in SLAM in dynamic environments (going back for example to Map building with mobile robots in dynamic environments, Haehnel et al IROS 2003 using lidar, and more recent efforts using vision such as used in the DARPA SubT challenge, e.g. Kimera DSG). But there is still more to be done, and in particular for large scale outdoor scenarios of Army interest using monocular inputs.    
    }

    Thanks! We are equally excited to work on this challenging project with monocular inputs.

    %===============================================================================
    \textbf{\revFour, Q6: The proposal has substantial technical details on the proposed approaches, which should lead to effective and publishable contributions in this challenging domain.
    }

    Thanks! We would definitely want to be the leaders in this field.

    %===============================================================================
    \textbf{\revFour, Q7: As a suggestion, the PI should potentially consider some of the multi agent SLAM work being performed under DCIST, such as the work of Vijay Kumar's lab (UPenn), Luca Carlone (MIT) and other work such as GTSAM based efforts (Georgia Tech, Skdio etc). There may be some previously developed open source capabilities related to pose graph SLAM that can help the effort.
    }

    We plan to compare our methods with these methods on benchmarks. 

    %===============================================================================
    \textbf{\revFour, Q8: Evaluation will be critical; for Evaluation it would be good to go beyond \kitti and Cityscapes there are just so many papers that have used them, perhaps the field is a bit stuck in a local minimum. Perhaps the PIs can collect new data sets.
    }

    We also benchmark on a number of challenging datasets with diverse environments. 
    We list out these datasets in \cref{tab:datasets} and detail our response in \revThree, Q1.

    % We also report results on challenging subsets of Waymo (such as rain or dust) and evaluate in the cross-dataset setting on newer benchmarks. 
    % To additionally test robustness, we also plan to test monocular \threeD detectors trained on standard datasets on challenging datasets such as \advWeather \cite{bijelic2020seeing} and \ithaca \cite{diaz2022ithaca365}. Also see \revThree, Q1.

%============================================================================
%============================================================================
%============================================================================
{\small
\bibliographystyle{ieee_fullname}
\bibliography{references}
}


\end{document}